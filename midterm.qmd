---
title: "Midterm Project"
author: "Thomas Schittina"
toc: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
    embed-resources: true
    self-contained-math: true	
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
---

## Data Cleaning

### (a) Import Data

After reading in the dataset, we rename the columns in accordance with standard 
naming conventions. Next, we convert the datatype of a few columns we know will 
be important later. The first two columns, `created_date` and `closed_date`, 
are converted to datetime objects, and `incident_zip` is recast to contain 
strings. For the sake of the conversion, any `NaN` in `incident_zip` is 
replaced by the value 0, which we will handle later.

```{python}
import pandas as pd
import numpy as np

# read coordinates in as 32-bit float
df = pd.read_csv('data/nycflood2024.csv',
                 dtype={'Latitude': np.float32, 'Longitude': np.float32,})

# ensure columns comply with naming conventions
df.columns = df.columns.str.lower().str.replace(' ', '_')

# convert to datetime objects
df['created_date'] = pd.to_datetime(df['created_date'],
                                    format='%m/%d/%Y %I:%M:%S %p', 
                                    errors='coerce')
df['closed_date'] = pd.to_datetime(df['closed_date'],
                                   format='%m/%d/%Y %I:%M:%S %p', 
                                   errors='coerce')

# convert zip codes to strings, convert NaN to zero (fill later)
df['incident_zip'] = df['incident_zip'].fillna(0).astype(int).astype(str)
```

### (b) Summarize missing information

The method `df.info()` is used to summarize missing information. It reveals 
that 11 columns are completely null. By referencing the website's explanation 
of these missing variables, we see that none of them pertain to flood data. 
Therefore, it is safe to drop these columns; they will not provide any insight 
or predictive power to our model.

```{python}
# drop entirely null columns
df = df.dropna(axis=1, how='all')
```

This leaves us with 30 columns, most of which are missing little to no 
information.

### (c) Redundant information and feather format

Any column consisting of one unique value will not provide any insight we can 
leverage in our model. Their lack of variation makes them redundant.

```{python}
# identify and drop columns with no variation
to_drop = list(df.columns[df.nunique() == 1])
df = df.drop(columns=to_drop)
```

Now, we can reference the website's data dictionary to identify other columns 
that may be unnecessary. Our search indicates `street_name` may be redundant 
given we already have `incident_address`. Further inspection reveals this to be 
the case. We also find that much of the geographic/location data is repeated in 
various columns. We will drop all of these redundant variables.

```{python}
# drop other redundant columns
to_drop = ['street_name', 'x_coordinate_(state_plane)',
           'y_coordinate_(state_plane)', 'bbl', 'park_borough', 'location']
df = df.drop(columns=to_drop)
```

Next, we'd like to examine the efficiency gain from using the Arrow feather 
format compared to csv. The feather file is created by reading in the data from 
the csv and storing it in a new dataframe. Then, 
`df.to_feather('data/nycflood2024.feather')` creates the file. The original csv 
is **5.82 MB** and `nycflood2024.feather` is **1.85 MB**. The feather format is 
*68%* smaller than the csv, making it far more efficient for storage.

### (d)

Inspecting `borough` we find that there are two missing entries. We also recall 
from part (a) that four missing zip codes were set to 0. However, it turns out 
that this missingness is unrelated.

```{python}
# lookup borough information for missing zip codes
df.loc[(df['incident_zip'] == '0'), ['incident_zip', 'borough']]
```

This means we can easily find the missing borough information using the intact 
zip codes. Both rows where `borough` is unspecified correspond to a zip code of 
11208. Searching our dataframe, we find that this zip code is a part of 
Brooklyn. We update the dataframe accordingly.

```{python}
# fill in borough data
df.loc[(df['borough'] == 'Unspecified'), ['borough']] = 'BROOKLYN'
```

Next, we try to fill in the missing zip codes using `latitude` and `longitude`, 
but there is a problem.

```{python}
df.loc[(df['incident_zip'] == '0'), ['latitude', 'longitude']]
```

The four missing zip codes correspond to the four missing coordinates. Without 
geocodes, there is no way to recover the zip codes. Since our model will rely 
on zip code specific information, we will drop these rows.

```{python}
# drop rows with unrecoverable zip codes
df = df.drop(index=[58, 189, 1903, 6333])
```

### (e) Date errors

The values in `created_date` should be strictly less than those in 
`closed_date` because it must take time for the complaint to be resolved. 
However, `(df['created_date'] >= df['closed_date']).sum()` indicates there are 
161 observations where this inequality fails. In 160 of these observations, the 
created and closed times are exactly equal; there is one case where the 
`created_date > closed_date`. These errors might be explained by a default 
procedure for data entry when the start or end time is lost. If one time is 
missing and cannot be estimated, it may be automatically set to the value of 
the other. Otherwise, such discrepancies are likely the result of some error. 
Regardless, these observations will not be useful for modeling response time. 
They are unrealistic and introduce bias to the model, so we will drop them.

```{python}
# identify and drop rows with severe time errors
to_drop = df[(df['created_date'] >= df['closed_date'])].index
df = df.drop(index=to_drop)
```

We can investigate for other suspicious entries in `created_date` and 
`closed_date`. Let's look at dates reported exactly on the hour, down to the 
second.

```{python}
import datetime

# check for times reported exactly on the hour
df.loc[(df['created_date'].dt.minute == 0) & (df['created_date'].dt.second == 0), ['created_date']]
```

There are three such occurrences in `created_date`. There are many more in 
`closed_date`.

```{python}
# find number of times reported exactly on the hour
df.loc[(df['closed_date'].dt.minute == 0) & (df['closed_date'].dt.second == 0), ['closed_date']].shape
```

There are 1219 times reported exactly on the hour in `closed_date`. However, 
there is a probable explanation for this. Whenever the exact time a complaint 
is opened or closed is uncertain, it may be approximated be the nearest hour. 
These observations should not be harmful to the model, since they are still 
realistic. Therefore, they do not need to be dropped.

Although `created_date` is entirely non-null, there are 208 `NaT` entries in 
`closed_date`. These may be interpreted as service requests that have not been 
resolved yet. The status for all these observations is one of 'Open', 
'Started', or 'Assigned'. However, they may also be the result of an error 
in data entry. For now, we will leave them in the dataset.

```{python}
# get dataframe for entries with no close date
df.loc[df['closed_date'].isna(), ['created_date', 'status', 'descriptor']].head()
```

### (f) Suggestions to data curator

I have the following suggestions for the data curator:

+ Consider the feather format for more efficient storage. The full service 
  request dataset is very large, so file format is important.
+ Look to reduce the dimensionality of the dataset. Several of the variables 
  are repetitive. For example, storing the latitude and longitude as a 
  coordinate pair is redundant given there are already separate columns for 
  both.
+ Include additional details regarding data collection and entry where 
  appropriate. The data dictionary online is helpful, but there are certain 
  trends in the data that could be explained with a brief overview of how data 
  is gathered (like in part e, with the dates that occur exactly on the hour).

## Exploratory Analysis

### (a) Visualizing complaints

We'll use `gmplot` to overlay the complaints on a map of NYC. First though, the 
values in `descriptor` need to be reformatted.

```{python}
# rename complaint type values
df = df.replace({'Street Flooding (SJ)': 'SF',
                 'Catch Basin Clogged/Flooding (Use Comments) (SC)': 'CB'})
```

Now the column `descriptor` is easier to work with. Let's separate the 
coordinates for flood complaints from basin complaints.

```{python}
# create different lists of coordinates for SF and CB
flood_coords = df.loc[df['descriptor'] == 'SF', ['latitude', 'longitude']]
basin_coords = df.loc[df['descriptor'] == 'CB', ['latitude', 'longitude']]
flood_lats = flood_coords['latitude']
flood_lons = flood_coords['longitude']
basin_lats = basin_coords['latitude']
basin_lons = basin_coords['longitude']
```

Now, we're ready to visualize the location of SF and CB complaints using 
`gmplot`. On the map below, red points are street floods and blue points are 
catch basin incidents.

```{python}
import gmplot

# read in API key
apikey = open('gmapKey.txt').read().strip()

# overlay coordinate to map of NYC
gmap = gmplot.GoogleMapPlotter(40.7128, -74.0600, 11.5, apikey=apikey)
gmap.scatter(flood_lats.dropna(), flood_lons.dropna(), marker=False, color='red',
             size=60, fa=0.6)
gmap.scatter(basin_lats.dropna(), basin_lons.dropna(), marker=False, color='blue',
             size=60, fa=0.6)
gmap.draw('basin_flood_map.html')
```

![SF (red) and CB (blue) complaints in NYC, 2024](flood_basin_loc.png)

The map shows the relative frequency of the two events, compared to each other. 
It also highlights certain hotspots, like at the Rockaway Beach.

### (b) Constructing response time

Our goal is to model the time it takes for complaints to be resolved. So, we 
will need to introduce the variable `response_time`, constructed from 
`created_date` and `closed_date`.

```{python}
# create response time
df['response_time'] = df['closed_date'] - df['created_date']
df['response_time'].head()
```

### (c) Visualize response time by descriptor and borough

We can visualize `response_time` with histograms. First, lets look at the 
distribution of `response_time` for SF and CB separately.

```{python}
from plotnine import ggplot, aes, geom_histogram, facet_wrap

# convert to hours and transform for sake of graphing
df['response_time_hours'] = df['response_time'].apply(lambda x: x.total_seconds()) / 3600
df['response_time_hours_sqrt'] = df['response_time_hours'].apply(lambda x: np.sqrt(x))

# select columns to be referenced while plotting
hist_df = df[['response_time_hours_sqrt', 'descriptor', 'borough']]

(
    ggplot(hist_df.dropna(), aes('response_time_hours_sqrt'))
    + geom_histogram(binwidth=4, fill='skyblue')
    + facet_wrap('~descriptor')
)
```

Note that we transformed the response time in hours by its square root, to make 
the visuals more interpretable. The distribution of `response_time_hours_sqrt` 
resembles a gamma distribution. It quickly rises and then decays exponentially. 
The distribution is clearer if we remove the largest outliers and adjust the 
bin width.

```{python}
# select largest outliers
drop = list(df['response_time_hours_sqrt'].nlargest(3).index)

# create histogram for each descriptor
(
    ggplot(hist_df.drop(index=drop).dropna(), aes('response_time_hours_sqrt'))
    + geom_histogram(binwidth=2, fill='skyblue')
    + facet_wrap('~descriptor')
)
```

Next we'll look at the distributions for `response_time_hours_sqrt` across the 
different boroughs.

```{python}
# create histogram for each borough
(
    ggplot(hist_df.drop(index=drop).dropna(), aes('response_time_hours_sqrt'))
    + geom_histogram(binwidth=2, fill='skyblue')
    + facet_wrap('~borough')
)
```

Again, the distributions appear to be gamma/exponential. We see that Brooklyn 
and Queens get many more complaints than the rest of NYC. The distributions for 
the Bronx and Manhattan are flatter than the rest, and contain most of the 
outliers.

### (d) Hypothesis testing

We'd like to test three pairs of hypotheses at $\alpha = 0.05$:

1. $H_o$: The mean response time is the same for SF and CB<br>
   $H_a$: The mean response time for SF is different from CB

2. $H_o$: The mean response time is the same for all boroughs<br>
   $H_a$: At least one mean response time is different across the boroughs

3. $H_o$: There is no significant interaction between descriptor and borough<br>
   $H_a$: There is significant interaction between descriptor and borough

The corresponding test is a two-way ANOVA. This requires that each sample is 
normally distributed, which we suspect is not the case for the original data. A 
log transformation coerces the data to follow an approximately normal 
distribution, as shown below.

```{python}
# log transformation
df['response_time_hours_log'] = df['response_time_hours'].apply(lambda x: np.log(x))

# select columns to be referenced while plotting
hist_df = df[['response_time_hours_log', 'descriptor', 'borough']]

# plot by descriptor
(
    ggplot(hist_df.dropna(), aes('response_time_hours_log'))
    + geom_histogram(binwidth=1, fill='skyblue')
    + facet_wrap('~descriptor')
)

#plot by borough
(
    ggplot(hist_df.dropna(), aes('response_time_hours_log'))
    + geom_histogram(binwidth=1, fill='skyblue')
    + facet_wrap('~borough')
)
```

The package `statsmodels` is used to conduct the test.

```{python}
import statsmodels.api as sm
from statsmodels.formula.api import ols

# data for two-way ANOVA
test_df = df[['response_time_hours_log', 'descriptor', 'borough']]

# perform two-way ANOVA
model = ols('response_time_hours_log ~ C(descriptor) + C(borough)\
            + C(descriptor):C(borough)', data=test_df).fit()
result = sm.stats.anova_lm(model, type=2)

result
```

All three p-values are very small. Therefore, the data indicates all three null 
hypotheses should be rejected. We conclude the following:

+ Mean response time is different for SF and CB
+ Mean response time is not consistent across the five boroughs
+ The difference in mean response time for SF and CB is impacted by borough

Note that these conclusions hold for the log-transformed data. In simple terms, 
we have found evidence that response time does depend on the type and location 
of the service request. 