---
title: "Midterm Project"
author: "Thomas Schittina"
toc: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
    embed-resources: true
    self-contained-math: true	
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
---

## Data Cleaning

### (a) Import Data

After reading in the dataset, we rename the columns in accordance with standard 
naming conventions. Next, we convert the datatype of a few columns we know will 
be important later. The first two columns, `created_date` and `closed_date`, 
are converted to datetime objects, and `incident_zip` is recast to contain 
strings. For the sake of the conversion, any `NaN` in `incident_zip` is 
replaced by the value 0, which we will handle later.

```{python}
import pandas as pd
import numpy as np

# read coordinates in as 32-bit float
df = pd.read_csv('data/nycflood2024.csv',
                 dtype={'Latitude': np.float32, 'Longitude': np.float32,})

# ensure columns comply with naming conventions
df.columns = df.columns.str.lower().str.replace(' ', '_')

# convert to datetime objects
df['created_date'] = pd.to_datetime(df['created_date'],
                                    format='%m/%d/%Y %I:%M:%S %p', 
                                    errors='coerce')
df['closed_date'] = pd.to_datetime(df['closed_date'],
                                   format='%m/%d/%Y %I:%M:%S %p', 
                                   errors='coerce')

# convert zip codes to strings, convert NaN to zero (fill later)
df['incident_zip'] = df['incident_zip'].fillna(0).astype(int).astype(str)
```

### (b) Summarize missing information

The method `df.info()` is used to summarize missing information. It reveals 
that 11 columns are completely null. By referencing the website's explanation 
of these missing variables, we see that none of them pertain to flood data. 
Therefore, it is safe to drop these columns; they will not provide any insight 
or predictive power to our model.

```{python}
# drop entirely null columns
df = df.dropna(axis=1, how='all')
```

This leaves us with 30 columns, most of which are missing little to no 
information.

### (c) Redundant information and feather format

Any column consisting of one unique value will not provide any insight we can 
leverage in our model. Their lack of variation makes them redundant.

```{python}
# identify and drop columns with no variation
to_drop = list(df.columns[df.nunique() == 1])
df = df.drop(columns=to_drop)
```

Now, we can reference the website's data dictionary to identify other columns 
that may be unnecessary. Our search indicates `street_name` may be redundant 
given we already have `incident_address`. Further inspection reveals this to be 
the case. We also find that much of the geographic/location data is repeated in 
various columns. We will drop all of these redundant variables.

```{python}
# drop other redundant columns
to_drop = ['street_name', 'x_coordinate_(state_plane)',
           'y_coordinate_(state_plane)', 'bbl', 'park_borough', 'location']
df = df.drop(columns=to_drop)
```

Next, we'd like to examine the efficiency gain from using the Arrow feather 
format compared to csv. The feather file is created by reading in the data from 
the csv and storing it in a new dataframe. Then, 
`df.to_feather('data/nycflood2024.feather')` creates the file. The original csv 
is **5.82 MB** and `nycflood2024.feather` is **1.85 MB**. The feather format is 
*68%* smaller than the csv, making it far more efficient for storage.

### (d)

Inspecting `borough` we find that there are two missing entries. We also recall 
from part (a) that four missing zip codes were set to 0. However, it turns out 
that this missingness is unrelated.

```{python}
# lookup borough information for missing zip codes
df.loc[(df['incident_zip'] == '0'), ['incident_zip', 'borough']]
```

This means we can easily find the missing borough information using the intact 
zip codes. Both rows where `borough` is unspecified correspond to a zip code of 
11208. Searching our dataframe, we find that this zip code is a part of 
Brooklyn. We update the dataframe accordingly.

```{python}
# fill in borough data
df.loc[(df['borough'] == 'Unspecified'), ['borough']] = 'BROOKLYN'
```

Next, we try to fill in the missing zip codes using `latitude` and `longitude`, 
but there is a problem.

```{python}
df.loc[(df['incident_zip'] == '0'), ['latitude', 'longitude']]
```

The four missing zip codes correspond to the four missing coordinates. Without 
geocodes, there is no way to recover the zip codes. Since our model will rely 
on zip code specific information, we will drop these rows.

```{python}
# drop rows with unrecoverable zip codes
df = df.drop(index=[58, 189, 1903, 6333])
```

### (e) Date errors

The values in `created_date` should be strictly less than those in 
`closed_date` because it must take time for the complaint to be resolved. 
However, `(df['created_date'] >= df['closed_date']).sum()` indicates there are 
161 observations where this inequality fails. In 160 of these observations, the 
created and closed times are exactly equal; there is one case where the 
`created_date > closed_date`. These errors might be explained by a default 
procedure for data entry when the start or end time is lost. If one time is 
missing and cannot be estimated, it may be automatically set to the value of 
the other. Otherwise, such discrepancies are likely the result of some error. 
Regardless, these observations will not be useful for modeling response time. 
They are unrealistic and introduce bias to the model, so we will drop them.

```{python}
# identify and drop rows with severe time errors
to_drop = df[(df['created_date'] >= df['closed_date'])].index
df = df.drop(index=to_drop)
```

We can investigate for other suspicious entries in `created_date` and 
`closed_date`. Let's look at dates reported exactly on the hour, down to the 
second.

```{python}
import datetime

# check for times reported exactly on the hour
df.loc[(df['created_date'].dt.minute == 0) & (df['created_date'].dt.second == 0), ['created_date']]
```

There are three such occurrences in `created_date`. There are many more in 
`closed_date`.

```{python}
# find number of times reported exactly on the hour
df.loc[(df['closed_date'].dt.minute == 0) & (df['closed_date'].dt.second == 0), ['closed_date']].shape
```

There are 1219 times reported exactly on the hour in `closed_date`. However, 
there is a probable explanation for this. Whenever the exact time a complaint 
is opened or closed is uncertain, it may be approximated be the nearest hour. 
These observations should not be harmful to the model, since they are still 
realistic. Therefore, they do not need to be dropped.

Although `created_date` is entirely non-null, there are 208 `NaT` entries in 
`closed_date`. These may be interpreted as service requests that have not been 
resolved yet. The status for all these observations is one of 'Open', 
'Started', or 'Assigned'. However, they may also be the result of an error 
in data entry. For now, we will leave them in the dataset.

```{python}
# get dataframe for entries with no close date
df.loc[df['closed_date'].isna(), ['created_date', 'status', 'descriptor']].head()
```

### (f) Suggestions to data curator

I have the following suggestions for the data curator:

+ Consider the feather format for more efficient storage. The full service 
  request dataset is very large, so file format is important.
+ Look to reduce the dimensionality of the dataset. Several of the variables 
  are repetitive. For example, storing the latitude and longitude as a 
  coordinate pair is redundant given there are already separate columns for 
  both.
+ Include additional details regarding data collection and entry where 
  appropriate. The data dictionary online is helpful, but there are certain 
  trends in the data that could be explained with a brief overview of how data 
  is gathered (like in part e, with the dates that occur exactly on the hour).

## Exploratory Analysis

### (a) Visualizing complaints

We'll use `gmplot` to overlay the complaints on a map of NYC. First though, the 
values in `descriptor` need to be reformatted.

```{python}
# rename complaint type values
df = df.replace({'Street Flooding (SJ)': 'SF',
                 'Catch Basin Clogged/Flooding (Use Comments) (SC)': 'CB'})
```

Now the column `descriptor` is easier to work with. Let's separate the 
coordinates for flood complaints from basin complaints.

```{python}
# create different lists of coordinates for SF and CB
flood_coords = df.loc[df['descriptor'] == 'SF', ['latitude', 'longitude']]
basin_coords = df.loc[df['descriptor'] == 'CB', ['latitude', 'longitude']]
flood_lats = flood_coords['latitude']
flood_lons = flood_coords['longitude']
basin_lats = basin_coords['latitude']
basin_lons = basin_coords['longitude']
```

Now, we're ready to visualize the location of SF and CB complaints using 
`gmplot`. On the map below, red points are street floods and blue points are 
catch basin incidents.

```{python}
import gmplot

# read in API key
apikey = open('gmapKey.txt').read().strip()

# overlay coordinate to map of NYC
gmap = gmplot.GoogleMapPlotter(40.7128, -74.0600, 11.5, apikey=apikey)
gmap.scatter(flood_lats.dropna(), flood_lons.dropna(), marker=False, color='red',
             size=60, fa=0.6)
gmap.scatter(basin_lats.dropna(), basin_lons.dropna(), marker=False, color='blue',
             size=60, fa=0.6)
gmap.draw('basin_flood_map.html')
```

![SF (red) and CB (blue) complaints in NYC, 2024](flood_basin_loc.png)

The map shows the relative frequency of the two events, compared to each other. 
It also highlights certain hotspots, like at the Rockaway Beach.

### (b) Constructing response time

Our goal is to model the time it takes for complaints to be resolved. So, we 
will need to introduce the variable `response_time`, constructed from 
`created_date` and `closed_date`.

```{python}
# create response time
df['response_time'] = df['closed_date'] - df['created_date']
df['response_time'].head()
```

### (c) Visualize response time by descriptor and borough

We can visualize `response_time` with histograms. First, lets look at the 
distribution of `response_time` for SF and CB separately.

```{python}
from plotnine import ggplot, aes, geom_histogram, facet_wrap

# convert to hours and transform for sake of graphing
df['response_time_hours'] = df['response_time'].apply(lambda x: x.total_seconds()) / 3600
df['response_time_hours_sqrt'] = df['response_time_hours'].apply(lambda x: np.sqrt(x))

# select columns to be referenced while plotting
hist_df = df[['response_time_hours_sqrt', 'descriptor', 'borough']]

(
    ggplot(hist_df.dropna(), aes('response_time_hours_sqrt'))
    + geom_histogram(binwidth=4, fill='skyblue')
    + facet_wrap('~descriptor')
)
```

Note that we transformed the response time in hours by its square root, to make 
the visuals more interpretable. The distribution of `response_time_hours_sqrt` 
resembles a gamma distribution. It quickly rises and then decays exponentially. 
The distribution is clearer if we remove the largest outliers and adjust the 
bin width.

```{python}
# select largest outliers
drop = list(df['response_time_hours_sqrt'].nlargest(3).index)

# create histogram for each descriptor
(
    ggplot(hist_df.drop(index=drop).dropna(), aes('response_time_hours_sqrt'))
    + geom_histogram(binwidth=2, fill='skyblue')
    + facet_wrap('~descriptor')
)
```

Next we'll look at the distributions for `response_time_hours_sqrt` across the 
different boroughs.

```{python}
# create histogram for each borough
(
    ggplot(hist_df.drop(index=drop).dropna(), aes('response_time_hours_sqrt'))
    + geom_histogram(binwidth=2, fill='skyblue')
    + facet_wrap('~borough')
)
```

Again, the distributions appear to be gamma/exponential. We see that Brooklyn 
and Queens get many more complaints than the rest of NYC. The distributions for 
the Bronx and Manhattan are flatter than the rest, and contain most of the 
outliers.

### (d) Hypothesis testing of response time

We'd like to test three pairs of hypotheses at $\alpha = 0.05$:

1. $H_o$: The mean response time is the same for SF and CB<br>
   $H_a$: The mean response time for SF is different from CB

2. $H_o$: The mean response time is the same for all boroughs<br>
   $H_a$: At least one mean response time is different across the boroughs

3. $H_o$: There is no significant interaction between descriptor and borough<br>
   $H_a$: There is significant interaction between descriptor and borough

The corresponding test is a two-way ANOVA. This requires that each sample is 
normally distributed, which we suspect is not the case for the original data. A 
log transformation coerces the data to follow an approximately normal 
distribution, as shown below.

```{python}
# log transformation
df['response_time_hours_log'] = df['response_time_hours'].apply(lambda x: np.log(x))

# select columns to be referenced while plotting
hist_df = df[['response_time_hours_log', 'descriptor', 'borough']]

# plot by descriptor
(
    ggplot(hist_df.dropna(), aes('response_time_hours_log'))
    + geom_histogram(binwidth=1, fill='skyblue')
    + facet_wrap('~descriptor')
)

#plot by borough
(
    ggplot(hist_df.dropna(), aes('response_time_hours_log'))
    + geom_histogram(binwidth=1, fill='skyblue')
    + facet_wrap('~borough')
)
```

The package `statsmodels` is used to conduct the test.

```{python}
import statsmodels.api as sm
from statsmodels.formula.api import ols

# data for two-way ANOVA
test_df = df[['response_time_hours_log', 'descriptor', 'borough']]

# perform two-way ANOVA
model = ols('response_time_hours_log ~ C(descriptor) + C(borough)\
            + C(descriptor):C(borough)', data=test_df).fit()
result = sm.stats.anova_lm(model, type=2)

result
```

All three p-values are very small. Therefore, the data indicates all three null 
hypotheses should be rejected. We conclude the following:

+ Mean response time is different for SF and CB
+ Mean response time is not consistent across the five boroughs
+ The difference in mean response time for SF and CB is impacted by borough

Note that these conclusions hold for the log-transformed data. In simple terms, 
we have found evidence that response time does depend on the type and location 
of the service request.

### (e) Constructing target variable

Ultimately, we want to predict whether a service request will take three or 
more days to be resolved. We need to construct our target variable `over3d`. 
If we intepret values of `NaT` as unresolved complaints, we can count them as 
taking more than three days.

```{python}
df['over3d'] = (df['response_time'].dt.days >= 3) | (df['response_time'].isna())
df['over3d'] = df['over3d'].astype(int)
```

### (f) Hypothesis testing of `over3d`

`over3d` is a categorical variable, so we'll need to construct contigency 
tables to perform the desired tests. First, lets look at the relationship 
between `over3d` and `descriptor`.

```{python}
# contingecy table of descriptor and over3d
descriptor_table = pd.crosstab(df['descriptor'], df['over3d'])
descriptor_table
```

The appropriate test is a chi-square test of independence. The test assumes 
there is at least five observations in each cell of the contingency table, 
which there are. The hypotheses are:

+ $H_o$: The variables `over3d` and `descriptor` are independent
+ $H_a$: The variables `over3d` and `descriptor` are dependent

Assume $\alpha = 0.05$.

```{python}
from scipy.stats import chi2_contingency

# chi-squared test
result = chi2_contingency(descriptor_table)
result.pvalue
```

The resulting p-value is much smaller than 0.05, so the data rejects the null 
hypothesis. We conclude that `over3d` depends on `descriptor`. This is an 
expected result, as earlier we saw that mean response time differs based on the 
type of complaint.

Next, we repeat a similar procedure for `over3d` and `borough`. First, we need 
a contingency table.

```{python}
# contingency table of borough and over3d
borough_table = pd.crosstab(df['borough'], df['over3d'])
borough_table
```

Assume $\alpha = 0.05$. The hypotheses are:

+ $H_o$: The variables `over3d` and `borough` are independent
+ $H_a$: The variables `over3d` and `borough` are dependent

```{python}
# chi-squared test
result = chi2_contingency(borough_table)
result.pvalue
```

A very small p-value once again indicates dependence between the variable. We 
reject the null hypothesis in favor of the claim that `over3d` and `borough` 
depend on one another.

## Modeling Occurence of Overly Long Response Time

### (a) Creating dataset for the model

Our model will include the following covariates:

+ `descriptor`
+ `borough`
+ `night` (occurred from 5pm to 5am)
+ `winter` (occurred from Nov 30 to March 1)
+ `pop_density`
+ `Unemployed` (count in incident zip code)
+ `Median Household Income` (in zip code)
+ `Median Home Value` (in zip code)

The dataset will be constructed from the service request data we've been using, 
`nyc_zip_areas.feather`, and `acs2023.feather`. First, let's read in the zip 
code and area data.

```{python}
# read in other datasets
acs = pd.read_feather('data/acs2023.feather')
areas = pd.read_feather('data/nyc_zip_areas.feather')

# merge on zip code
acs = acs.merge(areas, left_on='ZIP Code', right_on='modzcta', how='left')

# calculate population density
acs['pop_density'] = acs['Total Population'] / acs['land_area_sq_miles']

# fill missing population densities with median
median_density = acs['pop_density'].median()
acs.loc[:, ['pop_density']] = acs['pop_density'].fillna(median_density)

# select variables of interest
acs = acs[['Median Household Income', 'Unemployed', 'Median Home Value',
           'ZIP Code', 'pop_density']]
```

Now, let's go back to the original flood dataframe and construct `night` and 
`season`.

```{python}
# store hour and date service request was created separately
df['created_hour'] = df['created_date'].apply(lambda x: x.strftime('%H:%M:%S'))
df['created_day'] = df['created_date'].apply(lambda x: x.strftime('%m-%d'))

# create column to indicate if complaint occurred at night
df['night'] = ((df['created_hour'] < '05:00:00') | (df['created_hour'] >= '17:00:00'))
df['night'] = df['night'].astype(int)

# create column to indicate if complaint occurred during winter
df['winter'] = ((df['created_day'] < '03-01') | (df['created_day'] > '11-30'))
df['winter'] = df['winter'].astype(int)
```

Next, we'll select a subset of the flood dataframe and merge it with the ACS 
dataframe.

```{python}
# select covariates and target
model_df = df[['descriptor', 'incident_zip', 'borough',
               'night', 'winter', 'over3d']]

# merge with acs data
model_df = model_df.merge(acs, left_on='incident_zip', right_on='ZIP Code', 
                          how='left')
                        
# drop zip code columns after merge
model_df.drop(columns=['ZIP Code', 'incident_zip'], inplace=True)

# drop 30 rows with NaN, only 0.3% of total observations
model_df.dropna(axis=0, inplace=True)
```

Lastly, we need to convert `descriptor` and `borough` to numeric data via 
one-hot encoding.

```{python}
# one-hot encoding
model_df = pd.get_dummies(model_df, columns=['borough', 'descriptor'], 
                                             dtype=int)
```

Our dataset is ready for modeling.

### (b) Logistic regression

We'll reserve 20% of the data for testing the model. After the split, the 
features are transformed.

```{python}
from sklearn.model_selection import train_test_split

# create set of predictors and response
X = model_df.drop(columns=['over3d'])
y = model_df['over3d']

# 80-20 split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1234,
                                                    test_size=0.2)

# scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

Next, we build and fit a logistic regression with L1 regularization. The 
parameter `C` controls the strength of the L1 penalty, and is chosen via 
cross-validation. The parameter `class_weight` is set to accomodate 
imbalance in the data (minority class makes up ~25% of data).

```{python}
from sklearn.linear_model import LogisticRegression

# create and fit model
model = LogisticRegression(penalty='l1', solver='liblinear',
                           class_weight='balanced', max_iter=1000)

# cross-validation to choose 'C'
param_grid = {
    'C': np.logspace(-6, 6, 20)
}
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5,
                           scoring='f1', verbose=1, n_jobs=-1)
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

print('The optimal value of C was', grid_search.best_params_['C'])
```

### (c) Confusion matrix

We'll test our model on `X_train` and visualize the confusion matrix.

```{python}
from sklearn.metrics import (
    accuracy_score, 
    precision_score,
    recall_score,
    f1_score
)

# make predictions
yhat = best_model.predict(X_test)

# compute performance scores
accuracy = round(accuracy_score(y_test, yhat), 2)
precision = round(precision_score(y_test, yhat), 2)
recall = round(recall_score(y_test, yhat), 2)
f1 = round(f1_score(y_test, yhat), 2)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1: {f1}')
```

The model offers some predictive power (better than randomly guessing), but the 
performance metrics are relatively poor. This can be visualized in a confusion 
matrix.

```{python}
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# create confusion matrix
cm = confusion_matrix(y_test, yhat)

# plot confusion matrix
sns.heatmap(cm, annot=True, xticklabels=['Not Long', 'Long'], 
            yticklabels=['Not Long', 'Long'], cmap='flare', fmt='g',
            linewidth=1, linecolor='white')
plt.xlabel('Prediction')
plt.ylabel('Actual')
plt.show()
```

The confusion matrix reveals the main issue with the model. It over-predicts 
the minority class of `over3d`. That explains the large quantity in the upper 
right cell. In simple terms, the model:

+ Provides better predictions of overly long response time than you would have 
  from guessing randomly. However, it is not that much better.
+ Is decent at capturing all the instances of an overly long response time.
+ Predicts an overly long response time too often. When it predicts a 
  complaint will have a slow response, it is only right a third of the time.

### (d) ROC curve

The ROC curve and ROC AUC are other metrics to judge model performance. Let's 
plot the ROC curve and find ROC AUC.

```{python}
from sklearn.metrics import roc_curve, auc

# get ROC and ROC AUC values
pred = best_model.predict_proba(X_test)
roc = roc_curve(y_test, pred[:, 1])
roc_auc = auc(roc[0], roc[1])

# plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(roc[0], roc[1], color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # 45 degree line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()
```

The graph again shows the model outperforms a random classifier, represented by 
the gray horizontal line. However, ROC AUC is only 0.68, which is not very high 
(the area under the random classifier curve is 0.5). Put simply, the model 
classifies complaints only marginally better than random guessing would.

### (e) Significance of predictors

Since we standardized our data, the relative magnitude (absolute value) of each 
coefficient gives some insight into the importance of the corresponding 
predictor.

```{python}
# dataframe of predictor coefficients
predictors = model_df.drop(columns=['over3d']).columns
coef = pd.DataFrame(np.abs(best_model.coef_.ravel()), predictors)
coef
```

The lasso shrunk most of the coefficients to zero, meaning those predictors are 
not important. The table us the most important piece of information is whether 
or not the service request originates from Brooklyn. The coefficient is 
actually negative, so service requests in Brooklyn are less likely to take a 
long time to resolve.

### (f) Summary of logistic regression

The results of the logistic regression are summarized in simple terms below.

+ The model classifies response time better than randomly guessing, but only 
  marginally.
+ The model predicts a complaint will have a response time over three days too 
  often. However, this means it does an alright job of identifying all 
  instances of a long response time.
+ Location is the most important factor in predicting whether there will be an 
  overly long response time. Service requests from Brooklyn are especially 
  unlikely to have a long response time. Also, clogged basins generally have a 
  longer response time than street floods, which makes sense as they're less 
  serious.
+ Better predictors are needed to improve the predictive power of the model. 
  This means choosing a different set of predictors, or creating new, more 
  informative predictors from existing data.