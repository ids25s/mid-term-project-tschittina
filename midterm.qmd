---
title: "Midterm Project"
author: "Thomas Schittina"
toc: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
    embed-resources: true
    self-contained-math: true	
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
---

## Data Cleaning

### (a) Import Data

After reading in the dataset, we rename the columns in accordance with standard 
naming conventions. Next, we convert the datatype of a few columns we know will 
be important later. The first two columns, `created_date` and `closed_date`, 
are converted to datetime objects, and `incident_zip` is recast to contain 
strings. For the sake of the conversion, any `NaN` in `incident_zip` is 
replaced by the value 0, which we will handle later.

```{python}
import pandas as pd
import numpy as np

# read coordinates in as 32-bit float
df = pd.read_csv('data/nycflood2024.csv',
                 dtype={'Latitude': np.float32, 'Longitude': np.float32,})

# ensure columns comply with naming conventions
df.columns = df.columns.str.lower().str.replace(' ', '_')

# convert to datetime objects
df['created_date'] = pd.to_datetime(df['created_date'],
                                    format='%m/%d/%Y %I:%M:%S %p', 
                                    errors='coerce')
df['closed_date'] = pd.to_datetime(df['closed_date'],
                                   format='%m/%d/%Y %I:%M:%S %p', 
                                   errors='coerce')

# convert zip codes to strings, convert NaN to zero (fill later)
df['incident_zip'] = df['incident_zip'].fillna(0).astype(int).astype(str)
```

### (b) Summarize missing information

The method `df.info()` is used to summarize missing information. It reveals 
that 11 columns are completely null. By referencing the website's explanation 
of these missing variables, we see that none of them pertain to flood data. 
Therefore, it is safe to drop these columns; they will not provide any insight 
or predictive power to our model.

```{python}
# drop entirely null columns
df = df.dropna(axis=1, how='all')
```

This leaves us with 30 columns, most of which are missing little to no 
information.

### (c) Redundant information and feather format

Any column consisting of one unique value will not provide any insight we can 
leverage in our model. Their lack of variation makes them redundant.

```{python}
# identify and drop columns with no variation
to_drop = list(df.columns[df.nunique() == 1])
df = df.drop(columns=to_drop)
```

Now, we can reference the website's data dictionary to identify other columns 
that may be unnecessary. Our search indicates `street_name` may be redundant 
given we already have `incident_address`. Further inspection reveals this to be 
the case. We also find that much of the geographic/location data is repeated in 
various columns. We will drop all of these redundant variables.

```{python}
# drop other redundant columns
to_drop = ['street_name', 'x_coordinate_(state_plane)',
           'y_coordinate_(state_plane)', 'bbl', 'park_borough', 'location']
df = df.drop(columns=to_drop)
```

Next, we'd like to examine the efficiency gain from using the Arrow feather 
format compared to csv. The feather file is created by reading in the data from 
the csv and storing it in a new dataframe. Then, 
`df.to_feather('data/nycflood2024.feather')` creates the file. The original csv 
is **5.82 MB** and `nycflood2024.feather` is **1.85 MB**. The feather format is 
*68%* smaller than the csv, making it far more efficient for storage.

### (d)

Inspecting `borough` we find that there are two missing entries. We also recall 
from part (a) that four missing zip codes were set to 0. However, it turns out 
that this missingness is unrelated.

```{python}
# lookup borough information for missing zip codes
df.loc[(df['incident_zip'] == '0'), ['incident_zip', 'borough']]
```

This means we can easily find the missing borough information using the intact 
zip codes. Both rows where `borough` is unspecified correspond to a zip code of 
11208. Searching our dataframe, we find that this zip code is a part of 
Brooklyn. We update the dataframe accordingly.

```{python}
# fill in borough data
df.loc[(df['borough'] == 'Unspecified'), ['borough']] = 'Brooklyn'
```

Next, we try to fill in the missing zip codes using `latitude` and `longitude`, 
but there is a problem.

```{python}
df.loc[(df['incident_zip'] == '0'), ['latitude', 'longitude']]
```

The four missing zip codes correspond to the four missing coordinates. Without 
geocodes, there is no way to recover the zip codes. Since our model will rely 
on zip code specific information, we will drop these rows.

```{python}
# drop rows with unrecoverable zip codes
df = df.drop(index=[58, 189, 1903, 6333])
```

### (e) Date errors

The values in `created_date` should be strictly less than those in 
`closed_date` because it must take time for the complaint to be resolved. 
However, `(df['created_date'] >= df['closed_date']).sum()` indicates there are 
161 observations where this inequality fails. In 160 of these observations, the 
created and closed times are exactly equal; there is one case where the 
`created_date > closed_date`. These errors might be explained by a default 
procedure for data entry when the start or end time is lost. If one time is 
missing and cannot be estimated, it may be automatically set to the value of 
the other. Otherwise, such discrepancies are likely the result of some error. 
Regardless, these observations will not be useful for modeling response time. 
They are unrealistic and introduce bias to the model, so we will drop them.

```{python}
# identify and drop rows with severe time errors
to_drop = df[(df['created_date'] >= df['closed_date'])].index
df = df.drop(index=to_drop)
```

We can investigate for other suspicious entries in `created_date` and 
`closed_date`. Let's look at dates reported exactly on the hour, down to the 
second.

```{python}
import datetime

# check for times reported exactly on the hour
df.loc[(df['created_date'].dt.minute == 0) & (df['created_date'].dt.second == 0), ['created_date']]
```

There are three such occurrences in `created_date`. There are many more in 
`closed_date`.

```{python}
# find number of times reported exactly on the hour
df.loc[(df['closed_date'].dt.minute == 0) & (df['closed_date'].dt.second == 0), ['closed_date']].shape
```

There are 1219 times reported exactly on the hour in `closed_date`. However, 
there is a probable explanation for this. Whenever the exact time a complaint 
is opened or closed is uncertain, it may be approximated be the nearest hour. 
These observations should not be harmful to the model, since they are still 
realistic. Therefore, they do not need to be dropped.

Although `created_date` is entirely non-null, there are 208 `NaT` entries in 
`closed_date`. These may be interpreted as service requests that have not been 
resolved yet. The status for all these observations is one of 'Open', 
'Started', or 'Assigned'. However, they may also be the result of an error 
in data entry. For now, we will leave them in the dataset.

```{python}
df.loc[df['closed_date'].isna(), ['created_date', 'status', 'descriptor']].head()
```

### (f) Suggestions to data curator

I have the following suggestions for the data curator:

+ Consider the feather format for more efficient storage. The full service 
  request dataset is very large, so file format is important.
+ Look to reduce the dimensionality of the dataset. Several of the variables 
  are repetitive. For example, storing the latitude and longitude as a 
  coordinate pair is redundant given there are already separate columns for 
  both.
+ Include additional details regarding data collection and entry where 
  appropriate. The data dictionary online is helpful, but there are certain 
  trends in the data that could be explained with a brief overview of how data 
  is gathered (like in part e, with the dates that occur exactly on the hour).  